{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "becb2f73-fe8d-4863-a165-6fbd2eb2d969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import contractions\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a6528b0-8b4f-454b-b319-72c39f09b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d749de34-9e66-467a-aecf-0db576624314",
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_year_df = pd.read_csv(r\"C:\\Users\\20223661\\Downloads\\data\\birth_year.csv\")\n",
    "ext_int_df = pd.read_csv(r\"C:\\Users\\20223661\\Downloads\\data\\extrovert_introvert.csv\")\n",
    "feeling_thinking_df = pd.read_csv(r\"C:\\Users\\20223661\\Downloads\\data\\feeling_thinking.csv\")\n",
    "gender_df = pd.read_csv(r\"C:\\Users\\20223661\\Downloads\\data\\gender.csv\")\n",
    "judging_df = pd.read_csv(r\"C:\\Users\\20223661\\Downloads\\data\\judging_perceiving.csv\")\n",
    "nationality_df = pd.read_csv(r\"C:\\Users\\20223661\\Downloads\\data\\nationality.csv\")\n",
    "political_df = pd.read_csv(r\"C:\\Users\\20223661\\Downloads\\data\\political_leaning.csv\")\n",
    "sensing_int_df = pd.read_csv(r\"C:\\Users\\20223661\\Downloads\\data\\sensing_intuitive.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c92be2-7957-4586-bab4-4335f185894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_birth_year_df = birth_year_df.sample(n=1000)\n",
    "small_regex_birth_year_df = birth_year_df.sample(n=1000)\n",
    "small_normal_birth_year_df = birth_year_df.sample(n=1000)\n",
    "\n",
    "small_birth_year_df = small_birth_year_df.reset_index(drop=True)\n",
    "small_regex_birth_year_df = small_regex_birth_year_df.reset_index(drop=True)\n",
    "small_normal_birth_year_df = small_normal_birth_year_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5590cc6e-cb8d-4400-8a01-680e1dfaa090",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_regex_birth_year_df['post'] = small_regex_birth_year_df['post'].apply(lambda x: re.compile(f\"({re.escape(str(x))})\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5705802f-6ec3-483d-ae87-1748b894a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Expand contractions\n",
    "small_normal_birth_year_df['post'] = small_normal_birth_year_df['post'].apply(contractions.fix)\n",
    "\n",
    "# Step 2: Normalize Unicode to ASCII\n",
    "small_normal_birth_year_df['post'] = small_normal_birth_year_df['post'].apply(\n",
    "    lambda x: unicodedata.normalize('NFKD', x).encode('ASCII', 'ignore').decode('utf-8')\n",
    ")\n",
    "\n",
    "# Step 3: Remove extra spaces\n",
    "small_normal_birth_year_df['post'] = small_normal_birth_year_df['post'].apply(\n",
    "    lambda x: ' '.join(x.split())\n",
    ")\n",
    "\n",
    "# Step 4: Remove URLs\n",
    "small_normal_birth_year_df['post'] = small_normal_birth_year_df['post'].apply(\n",
    "    lambda x: re.sub(r\"(https|http)?:\\S*\", \"\", x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce461c3-b3c6-4cd4-9a79-05d35801a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')       # For tokenization\n",
    "# nltk.download('stopwords')   # For stopwords\n",
    "# nltk.download('punkt_tab')   # For tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5a2d778-1443-40f3-a6fb-9739b634989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords from the 'post' column\n",
    "small_normal_birth_year_df['post'] = small_normal_birth_year_df['post'].apply(\n",
    "    lambda text: ' '.join(\n",
    "        [word for word in word_tokenize(text) if word.lower() not in stop_words]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51077d4-9a03-44f1-98d0-bf9da471835b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "current_year = datetime.now().year\n",
    "\n",
    "# Create a temporary DataFrame with unique authors\n",
    "unique_authors = birth_year_df.loc[birth_year_df['auhtor_ID'].drop_duplicates().index]\n",
    "\n",
    "# Calculate ages in the temporary DataFrame\n",
    "unique_authors['age'] = current_year - unique_authors['birth_year']\n",
    "\n",
    "# Plot the age distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(unique_authors['age'], bins=10, edgecolor='black', alpha=0.7)\n",
    "plt.title('Age Distribution of Unique Authors')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.savefig(r\"C:\\Users\\20223661\\Downloads\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "402b54e1-6c18-4c5e-83cc-ffac7ae977a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ee84f7-3d50-4256-b93e-777ec7bb1066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and test sets\n",
    "X = small_normal_birth_year_df['post']\n",
    "y = small_normal_birth_year_df['birth_year']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a pipeline for TF-IDF + Ridge Regression\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=55000)),  # Experiment with max_features\n",
    "    ('regressor', Ridge())  # Ridge works well for regression; you can try others like RandomForestRegressor\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abd0187a-3d5c-4f52-814f-91b8008545bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge MAE: 7.61\n",
      "Random Forest MAE: 8.11\n"
     ]
    }
   ],
   "source": [
    "X = small_birth_year_df['post']\n",
    "y = small_birth_year_df['birth_year']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Ridge Regression Pipeline\n",
    "# ridge_pipeline = Pipeline([\n",
    "#     ('tfidf', TfidfVectorizer(max_features=10000)),\n",
    "#     ('ridge', Ridge(alpha=1.0))\n",
    "# ])\n",
    "\n",
    "# Random Forest Pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(max_features=15000)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=25, random_state=42))\n",
    "])\n",
    "\n",
    "# Cross-validate and compare\n",
    "pipelines = {'Ridge': ridge_pipeline, 'Random Forest': rf_pipeline}\n",
    "for name, pipeline in pipelines.items():\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, scoring='neg_mean_absolute_error', cv=5)\n",
    "    print(f\"{name} MAE: {-scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b609db58-6165-4b5a-b6f9-c8bab40f410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper to ensure scikit-learn compatibility\n",
    "class SklearnCompatibleXGBRegressor(XGBRegressor, BaseEstimator, RegressorMixin):\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        return super().fit(X, y, **kwargs)\n",
    "\n",
    "# Data Preparation\n",
    "X = small_birth_year_df['post']\n",
    "y = small_birth_year_df['birth_year']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Pipeline\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('to_dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)),  # Convert sparse to dense\n",
    "    ('xgb', SklearnCompatibleXGBRegressor(n_estimators=50, random_state=42))\n",
    "])\n",
    "\n",
    "# Cross-validate the pipeline\n",
    "scores = cross_val_score(xgb_pipeline, X_train, y_train, scoring='neg_mean_absolute_error', cv=5)\n",
    "print(f\"XGBoost Pipeline MAE: {-scores.mean():.2f}\")\n",
    "\n",
    "# Train and Test the Pipeline\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "y_pred = xgb_pipeline.predict(X_test)\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Test MAE: {test_mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccc4ccb8-013e-489a-8bf4-aaf476161bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AdamW\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e905b2ee-a82b-4792-b981-31a65dbacff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20223661\\.conda\\envs\\JBC090\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\20223661\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\20223661\\.conda\\envs\\JBC090\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 391062460.5000\n",
      "Epoch 2/3, Loss: 387728304.5000\n",
      "Epoch 3/3, Loss: 385984464.5000\n",
      "Test MAE: 1962.22\n"
     ]
    }
   ],
   "source": [
    "# Dataset preparation\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, targets, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"target\": torch.tensor(target, dtype=torch.float),\n",
    "        }\n",
    "\n",
    "# BERT-based Regression Model\n",
    "class BertRegressor(nn.Module):\n",
    "    def __init__(self, bert_model_name):\n",
    "        super(BertRegressor, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.fc(self.drop(pooled_output))\n",
    "\n",
    "# Sklearn-compatible BERT wrapper\n",
    "class SklearnBERTRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, bert_model_name=\"bert-base-uncased\", max_length=128, batch_size=16, epochs=3, lr=5e-5):\n",
    "        self.bert_model_name = bert_model_name\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.bert_model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = BertRegressor(self.bert_model_name).to(self.device)\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        dataset = TextDataset(X, y, self.tokenizer, self.max_length)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                targets = batch[\"target\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = self.criterion(outputs.squeeze(), targets)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        dataset = TextDataset(X, [0] * len(X), self.tokenizer, self.max_length)  # Dummy targets\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size)\n",
    "\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# Data Preparation\n",
    "X = small_birth_year_df[\"post\"]\n",
    "y = small_birth_year_df[\"birth_year\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and Evaluate\n",
    "bert_regressor = SklearnBERTRegressor(epochs=3, batch_size=8)\n",
    "bert_regressor.fit(X_train.tolist(), y_train.tolist())\n",
    "\n",
    "y_pred = bert_regressor.predict(X_test.tolist())\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Test MAE: {test_mae:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
